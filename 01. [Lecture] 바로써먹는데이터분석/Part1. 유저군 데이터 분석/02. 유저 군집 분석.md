# Part1. 유저군 데이터 분석



## 실전프로젝트2)

### 유저 군집 분석



#### 1. 클러스터링 이론

* 데이터가 주어졌을 때, 여러 개의 그룹으로 나누는 것
* 유사한 특성을 가진 그룹을 발견해내는 일
* 내부 멤버들 간의 사이 (Intra-cluster)는 가깝고, 그룹 간 사이 (Inter-cluster)는 멀게



* 유사한 유저군 나누기: 마켓 세분화
  * 각 시장, 타겟 별로 효과적인 정책을 찾아낸다.

* SNS 관심사 기반 클러스터링
* 자율 주행 이미지 인식 클러스터링



* 클러스터링 종류

  * **Partition-based Clustering**
    * 미리 군집의 수를 정해두고 클러스터링하는 방식
    * <u>K-means</u>
      1. 사용자가 미리 군집수 (k) 정의
      2. 처음에는 랜덤으로 k개의 중심점(Centroid) 정한다.
      3. 가까운 중심점이 있는 그룹에 각 데이터 할당 (이 때 각 개체 간 거리는 Euclidean distance 사용)
      4. 모두 그룹을 할당했다면, 각 그룹마다 새 중심점(Centroid)- 클러스터 내 평균점을 새로 구한다.
      5. 3-4 단계 반복 후 더 이상 그룹 이동이 일어나지 않으면 멈춘다.

  * **Hierarchical based Clustering**
    * 여러 개의 군집 중에서 가장 유사도가 높은 혹은 거리가 가까운 군집 두 개를 선택해 하나로 합치면서 군집 개수를 줄여 가는 방법 (agglomerative clustering, 합체 군집)
    * 군집 간 거리 계산
      * <u>Centroid Distance</u>
        * 각 군집의 중심점 사이의 거리 계산
        * 계층 클러스터링이 아니더라도 사용 가능
      * <u>Median Distance</u>
        * 계층클러스터링에서 사용 가능
        * 군집 a와 군집 b가 결합해 군집 c가 생성되었다면, 군집 c의 중심점을 새로 계산하는 것이 아니라, 기존 a와 b의 중심점의 평균 사용 - 더 빨리 계산 가능
  * **Density based Clustering**
    * 데이터가 밀집한 정도, 밀도를 이용한 방법
    * <u>DBSCAN Clustering</u>
      * 군집의 개수를 지정할 필요 X
      * 초기 데이터로부터 근접한 데이터를 찾아나간다.
      * 초기 파라미터로 최소거리 a, 최소 데이터 개수 b가 필요
      * 최소 거리 a 안에 있는 데이터는 이웃이다.
      * 최소 거리 a 안에 최소 데이터 개수 b 이상의 데이터가 있으면, 이 데이터를 core로 정의
      * Core 데이터는 하나의 클러스터를 형성, 그 core와 a 거리 내에 있는 점들은 같은 클러스터로 분류
      * K-means는 원의 형태로 군집이 만들어지지만, 이는 불특정한 모양의 클러스터가 형성

​				

#### 정답이 없는 클러스터링, 어떻게 평가할 수 있을까?

* **Silhouette Coefficient, 실루엣 계수**

* 모든 데이터 쌍 (i,j)에 대한 거리나 dissimilarity

  * a_i: i와 같은 군집에 속한 원소들의 평균 거리

  * b_i: i와 다른 군집 중 가장 가까운 군집까지의 평균 거리

  * $$
    S_i = \frac{b_i-a_i}{max(a_i, b_i)}
    $$

  * 만약 a 같은 군집 내 평균 거리가 더 가깝다면 양수, 다른 군집과의 거리가 가깝다면 음수가 나온다.



#### 02. 상관계수

**기초 통계 개념**

1. **Variance**

   * 분산과 표준편차는 데이터가 얼마나 넓게 퍼져있는지 나타내는 값

   * 편차의 제곱합 / 데이터의 수

2. **표준편차**

   * 분산의 양의 제곱근
   * 분산에 루트를 씌우는 것은 제곱하면서 증가했었던 값을 다시 원래 단위로 맞추는 과정

3. **공분산**

   * 여러 필드 사이의 상관관계를 살펴보는 값

   * 예측한 추세선에 데이터가 가까울수록, X와 Y의 관계는 강하다.

   * $$
     cov_{xy} = \frac{\sum_{i=1}^{n}(x_i-x) (y_i-y)}{n}
     $$

   * 값이 양수이면 양의 관계, 음수면 음의 관계, 0에 근사하면 선형관계가 없음

   * 공분산을 직관적으로 알 수 있게 -1 ~ 1 사이의 값으로 스케일 조절한 값 -> **상관계수**

4. **상관계수**

   * 공분산은 값의 스케일이 크면 무한히 커진다.
   
   * $$
     Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{V(X)V(Y)}} = \frac{\sigma_{xy}}{\sigma_x \sigma_y}
     $$
   
   * 선형의 관계가 아닌 관계는 파악할 수 없다.



#### 03. 차원 축소
**차원의 저주**

* Feature들이 늘어나면 학습에 소요되는 시간과 비용이 커진다.

* 데이터 간 빈 공간이 많이 생긴다.

* 2-3차원으로 축소해야 시각화가 편하다.

* **Feature Selection** : feature의 중요도나 랭킹을 매겨, feature의 subset을 추출 또는 불필요한 feature 제거

* **Feature Extraction** : 기존 feature를 조합해 새로운 특징 생성

* **PCA** (Principal Component Analysis)

  * 고차원의 데이터를 저차원의 데이터로 환원시키는 기법
  * 다양한 표현 - 데이터 간 거리 - 데이터 간 편차를 잘 유지시키는 선을 찾는 것
  * 분산이 잘 보존되는 선을 찾아서, 그 선에 점들을 투영시킨다. (첫번째 선)
  * 첫번째 선과 직교하는 선을 찾으면 그것이 두번째 선이 된다.

* **공분산 행렬**

  * feature들의 변동, 즉 얼마만큼이나 함께 변하는가를 행렬로 표현
  * 데이터를 잘표현하고 있는 행렬, 대칭이다.

* Eigenvector, eigenvalue

  * $$
    Av = \lambda v
    $$

  * 고유값 labmda가 제일 큰 공분산 행렬의 eigenvector가 주성분이다.

    * A: 공분산 행렬(데이터의 분산을 표현)
    * v: 데이터가 어떤 방향으로 분산되어 있는지
    * Lambda: 고유값으로, 얼마나 그 공간이 퍼지는지(크기) 표현
