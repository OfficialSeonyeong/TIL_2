# Part 5. 고객 이탈 분석

## 실전 프로젝트 10)
### 이탈 예측 성능 끌어올리기

#### 01. Project Overview
* 음악 구독 서비스 구독 취소 데이터
    * 유저 성별, 나이, 도시, 유입 경로, 구매 내역 등
* 트리 앙상블 모델
    * 배깅(Bagging)
    * 부스팅(Boosting)


#### 02. 서비스 구독 취소 데이터 살펴보기
* train.csv
    * 유저ID - 이탈여부
* member.csv
    * 유저 성별, 나이, 도시 등 기본 정보
* transactions.csv
    * 구매 이력, 지불 방법, 자동 결제 여부 등
    * 다른 데이터와 다르게 유저 하나 당 여러 개의 데이터가 있을 수 있다.


#### 03. 트리 앙상블 이론 1 - 랜덤 포레스트
* 앙상블(ensenble) - 여러 개의 모델들을 조합하여 더 강력한 모델을 만든다.

* 트리 앙상블 모델
    * **배깅(Bagging)**
    * 부스팅(Boosting)

##### 부트스트랩(Bootstrap)
* 데이터에서 무작위로 부분집합을 뽑는다.
* N'개의 샘플을 N개의 샘플에서 반복적으로 뽑는다.
* 같은 데이터를 또 뽑아도 된다. (with replacement)
* 비슷하지만 서로 조금씩 다른 데이터셋이 만들어진다.
* When do we use Bootstrap?
    * 데이터셋이 달라질 때 얼마나 민감하게 변하는지 알고 싶을 때
    * 이 때 Bootstrap된 데이터셋으로 각각 모델을 학습시키고, 예측값의 분산(또는 표준편차)를 구하면 된다.

##### 배깅(Bagging)
* = Bootstrap Aggregating

1. Bootstrap를 K번 반복한다.
2. N' < N개의 샘플을 가진 학습용 데이터셋을 만든다.
3. 그 데이터셋에 분류기(classifier) 학습
4. 결과의 평균 도출

* 의사 결정 나무는 high variance, 즉 데이터셋에 따라 모델이 크게 달라진다.
* *반면, 선형 회귀 모델은 low variance*
* Bagging은 모델의 variance를 줄이는 기법으로, 특히 의사 결정 나무에 자주 쓰인다.

* bootstrap 데이터셋마다 의사 결정 나무를 학습시켜서 결과값을 평균 낸다.
* 의사 결정 나무 각각은 깊게 자라도록 놔둔다.
* 각각의 의사 결정 나무는 high variance를 가지게 되고, 평균 내면 variance가 줄어든다.
* 분류 문제의 경우 다수결로 결정

* 하지만! Bagging에서도 여전히 문제점이 존재한다.
* 만약 데이터에 하나의 강력한 설명변수가 있다고 가정하면, 대부분의 나무들은 최상단 분할에서 이 강력한 설명 변수를 이용하게 되고, 서로 상당히 유사하게 된다. 이런 경우 평균해도 하나의 트리를 만드는 것과 유사하게 된다!
* 이를 랜덤 포레스트가 해결한다.

##### 랜덤 포레스트(Random Forests)
* Bootstrap된 학습 데이터셋 이용하면서, 나무들의 상관성을 더욱 없애기 위해 (decorrelates) 추가적인 변형을 준다.
* **모든 설명변수를 사용하지 않고 일부분만 사용**(행뿐만 아니라 열도 샘플링)
* 나무가 분할(split)할 떄마다 무작위로 정한 m개의 설명 변수 중 분할할 설명 변수 고를 수 있음.
* 보통 m은 전체의 설명 개수의 제곱근으로 정한다.
* 이처럼 나무드르이 상관성을 없앰으로써, 나무들이 합쳤을 때 변동성이 작아지고(less variable), 그래서 더 안정적이게(more reliable) 된다.


#### 03. 트리 앙상블 이론 2 - 부스팅 모델
* 트리 앙상블 모델
    * 배깅(Bagging)
    * **부스팅(Boosting)**
        * 배깅은 의사 결정 나무들이 서로 독립적, 병렬적으로 학습
        * 부스팅은 나무들을 순차적으로 학습
        * 부스팅은 bootstrap sampling를 이용하지 않는다.
        * 현재의 모델에서 틀린 부분(residual)에 따른  따른 wegiht의 가중치를 계속 수정해 다음 의사 결정 나무 학습
        * 세가지 tuning parameters(hyperparameters)
            1. 트리의 개수
            - 트리의 개수가 많아지면 과적합될 우려가 있다.
            2. shrinkage parameter
            - 부스팅 모델이 얼마나 빠르게 학습할지 결정
            3. 각각 나무마다 분할의 개수
            - 트리 복잡도 결정